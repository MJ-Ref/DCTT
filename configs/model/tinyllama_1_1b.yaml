# TinyLlama-1.1B Model Configuration (Llama-family open fallback)
name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
revision: "main"
family: "llama"

# Embedding extraction
embedding_source: "model.embed_tokens.weight"
embedding_dim: 2048
vocab_size: 32000

# Model loading
torch_dtype: "float16"
trust_remote_code: false
low_cpu_mem_usage: true

use_mlx: false

# Tokenizer
tokenizer:
  trust_remote_code: false
  use_fast: true
